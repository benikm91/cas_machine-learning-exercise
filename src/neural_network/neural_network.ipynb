{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Aufgabenblatt 4\n",
    "\n",
    "`sklearn` hat auch ein Neural Network Modell (https://scikit-learn.org/stable/modules/neural_networks_supervised.html), in der Praxis werden aber oft ausgereiftere `Deep Learning` Libraries verwendet, wie `tensorflow` (Google) oder `PyTorch` (Facebook).\n",
    "\n",
    "Darum verwenden wir für diese Aufgabe `tensorflow` und nicht `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# random seed fixieren für Musterlösung \n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir Dinge von den vorherigen Aufgabenblätter nicht zu fest wiederholen, haben wir hier bereits den `Datensatz` und eine `Baseline` vorbereitet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz - MNIST\n",
    "\n",
    "Wir können den Datensatz über `sklearn.datasets.fetch_openml` laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "y = y.astype(np.int32) # Cast string like '1' to integer like 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir skalieren die Pixelwerte zwischen 0 und 1 (anstatt 0 und 255). Dies ist wichtig für das `Gradient Descent` Lernverfahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Vorher:\", X.max())\n",
    "X = X / 255  # Sehr einfaches skalieren (oft aussreichend für Bilder)\n",
    "print(\"Nach einfachem Skalieren:\", X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend teilen wir die Daten wieder in `Train-Set`, `Validation-Set` und `Test-Set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_data, X_test, y_data, y_test = train_test_split(X, y, test_size=5_000, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=5_000, stratify=y_data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und wir schauen uns Beispiele dieser handgeschriebenen Zahlen vom `Data-Set` an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(10., 10.))\n",
    "\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "    nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "    axes_pad=0.1,  # pad between axes in inch.\n",
    ")\n",
    "\n",
    "for ax, im in zip(grid, X_data.reshape(-1, 28, 28)):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(im, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie es bei der `Classification` eigentlich immer Sinn gibt, schauen wir uns die Verteilung der Zielvariable an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und sehen, dass wir in etwa gleich viele Datenpunkt pro Klasse haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Als `Baseline` haben wir hier bereits eine `Logistische Regression` analog zum Aufgabenblatt 3 trainiert und evaluiert.\n",
    "\n",
    "Wir nehmen allen Pixeln unverändert als Features und fitten das Modell darauf.\n",
    "\n",
    "Dieses `Baseline`-Modell erreicht eine Genauigkeit von ungefähr 92% - ein bereits ziemlich guter Wert, den wir aber in Aufgabe 2 verbessern werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_val_hat = clf.predict(X_val)\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie im Aufgabenblatt 3 können wir für diese `Baseline` die `Confusion Matrix` betrachten.\n",
    "\n",
    "Wir sehen, dass es oft zu Verwechslungen kommt zwischen `7` und `9` oder `5` und `8`. Manche Verwechslungen treten nie auf wie zwischen `0` und `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(y_val_hat, y_val, labels=clf.classes_),\n",
    "        columns=[clf.classes_], # Name columns\n",
    "        index=[clf.classes_] # Name rows\n",
    "    ),\n",
    "    annot=True # Show numbers in heatmap (not just colors)\n",
    ")\n",
    "plt.ylabel('True label') # Name y-axis\n",
    "plt.xlabel('Predicted label') # Name x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Folgende Funktion müssen Sie in den Extra-Aufgaben verwenden. Die Umsetzung (der Code) der Funktion muss **nicht** verstanden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt, floor, ceil\n",
    "\n",
    "def plot_weights(weights: np.array, f = None, alphas=None, colors=None, fig=None):\n",
    "\n",
    "    if alphas is None:\n",
    "        alphas = itertools.cycle([1]) \n",
    "    \n",
    "    if colors is None:\n",
    "        colors = itertools.cycle(['viridis'])\n",
    "    \n",
    "    if f is None:\n",
    "        f = lambda x: x\n",
    "        \n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=(18, 18))\n",
    "    \n",
    "    def get_2d_dim_of(size):\n",
    "        n_rows = floor(sqrt(size))\n",
    "        n_cols = ceil(sqrt(size))\n",
    "        if n_rows * n_cols < size:\n",
    "            n_rows = n_rows + 1\n",
    "        return n_rows, n_cols\n",
    "\n",
    "    size = weights.shape[1]\n",
    "    n_rows = floor(sqrt(size))\n",
    "    n_cols = ceil(sqrt(size))\n",
    "    size = weights.shape[0]\n",
    "    img_n_rows = floor(sqrt(size))\n",
    "    img_n_cols = ceil(sqrt(size))\n",
    "    if n_rows * n_cols < size:\n",
    "        n_rows = n_rows + 1\n",
    "\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "        nrows_ncols=get_2d_dim_of(weights.shape[1]),  # creates 2x2 grid of axes\n",
    "        axes_pad=(0.1, 0.3),\n",
    "        share_all=True,\n",
    "        cbar_location='right',\n",
    "        cbar_mode='single',\n",
    "        direction = 'row',\n",
    "        cbar_size='10%',\n",
    "        cbar_pad=0.1,\n",
    "    )\n",
    "\n",
    "    vmin, vmax = weights.min(), weights.max()\n",
    "    for i, (ax, cax, w, color, alpha) in enumerate(zip(grid, grid.cbar_axes, np.rollaxis(weights, 1), colors, alphas)):\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f(i))\n",
    "        im = ax.imshow(w.reshape(*get_2d_dim_of(weights.shape[0])), alpha=alpha, vmin=vmin, vmax=vmax, cmap=color)\n",
    "        # Plot colorbar for im (due to fixed vmin and vmax same for all images)\n",
    "        cb = plt.colorbar(im, cax=cax)\n",
    "        cb.set_alpha(1)\n",
    "        cb.draw_all()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aufgabe 1 - Logistische Regression als Neural Network\n",
    "\n",
    "In Aufgabe 1 bauen wir die `Logistische Regression` (die `Baseline` aus der Aufgabenbeschreibung) als `Neurales Netzwerk` nach. Anschliessend trainieren und evaluieren wir das Netzwerk.\n",
    "\n",
    "Dieses `Neurales Netzwerk` hat noch keinen `Hidden Layer` und ist dadurch ein lineares Modell. Es entspricht (bis auf wie es trainiert wird) der `Logistischen Regression` von `sklearn` und wir erwarten daher in Aufgabe 1 (noch) keine Verbesserung zur `Baseline`.\n",
    "\n",
    "In der Aufgabe 2 schauen wir uns dann ein `Neurales Netz` mit einem `Hidden Layer` mit nicht linearer Aktivierungsfunktion an - ein nicht lineares Modell.\n",
    "\n",
    "### Aufgabe 1.1 - Logistische Regression als (Feed Forward) Neural Network\n",
    "\n",
    "1. Erstellen Sie ein sequenzielles Neurales Netzwerk mittels `tf.keras.Sequential`\n",
    "2. Fügen Sie einen Input Layer `tf.keras.layers.InputLayer` mit der shape `input_shape=(28*28,)` hinzu mittels `model.add`.\n",
    "3. Fügen Sie einen Output Layer `tf.keras.layers.Dense` mit `units=10` und der Softmax Aktivierungsfunktion `activation=tf.keras.activations.softmax` hinzu mittels `model.add`.\n",
    "4. Geben Sie eine Beschreibung des Modell mittels `print(model.summary())` aus. Das Modell sollte 7850 totale Parameter haben.\n",
    "5. (Extra) Rechnen Sie die Anzahl Parameter von Hand nach.\n",
    "\n",
    "Zur Illustration ist das Neurale Netzwerk von Aufgabe 1 hier grafisch abgebildet. Es hat 784 Inputs ($x_1$, ..., $x_{784}$) für jeden Pixel ein Input (28 * 28 = 784) und 10 Ouputs ($\\hat y_0$, ..., $\\hat y_9$) für jede `Klasse` (Ziffer) einen Output.\n",
    "\n",
    "![Logistische Regression als Neural Network](./img/logistic-regression-as-nn-mnist.png)\n",
    "\n",
    "#### Hilfreiche Links\n",
    "\n",
    "* tf.keras.Sequential: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.2 - `model.compile`, `model.fit`\n",
    "\n",
    "In Aufgabe 1.1 haben wir das Netzwerk erstellt und mit `model.summary()` überprüft.\n",
    "\n",
    "Nun möchten wir das Modell trainieren (`fit`).\n",
    "\n",
    "Dazu müssen wir dem Modell mitteilen, welches Optimierungsverfahren (`optimizer`) und welche Kostenfunktion (`loss`) wir für das Training verwenden möchten.\n",
    "\n",
    "In der `Logistischen Regression` von `sklearn` sind diese Werte im Modell hard-codiert vorgegeben und man kann sie nicht überschreiben. \n",
    "`Neurale Netze` von `tensorflow` können mit unterschiedlichen Kostenfunktionen (`tf.keras.losses`) und unterschiedlichen Optimiertungsverfahren (z.B. `adam`) trainiert werden. Darum muss man diese Werte für ein `tensorflow` Modell noch angeben.\n",
    "\n",
    "1. Erstellen Sie eine `tf.keras.losses.SparseCategoricalCrossentropy` Funktion mit `from_logits=False`.\n",
    "2. Kompilieren Sie das Modell mit `model.compile`. Geben Sie die `loss` Funktion und den `optimizer` an. Für den `optimizer` können Sie `sgd` für das `Batch Gradient Descent` Verfahren oder `adam` für `Adam` ein verbessertes `Batch Gradient Descent` Verfahren.\n",
    "3. Trainieren Sie das Modell mittels `model.fit`. Wir müssen eine Batch Size `batch_size=64` und die Anzahl Epochen `epochs=15` mitgeben:\n",
    "    - `batch_size`: Wie viele Datenpunkte werden für einen Schritt im `Batch Gradient Descent` Verfahren verwendet.\n",
    "    - `epochs`: Wie oft iterieren wir über das gesamte `Train-Set`.\n",
    "4. Berechnen Sie die Wahrscheinlichkeiten der Klassen `y_val_hat_prob` auf dem `Validation-Set` (`X_val`) mittels `model.predict`.\n",
    "    - Anders als die Logistische Regression gibt uns `model.predict` hier 10 Outputs, **die Wahrscheinlichkeiten** für jede `Klasse`.\n",
    "5. Holen Sie die tatsächliche Vorhersagen `y_val_hat` aus den Wahrscheinlichkeiten `y_val_hat_prob` aus Schritt 2. Die tatsächliche Vorhersage ist der Output mit dem grössten Wert (der grösste Wahrscheinlichkeit).\n",
    "    - Nutzen Sie dazu `np.argmax` über die zweite Achse `axis=1`.\n",
    "    - Geben Sie die `shape` von `y_val_hat_prob` und von `y_val_hat` aus, um besser zu verstehen, was wir genau machen.\n",
    "6. Berechnen Sie die Genauigkeit unseres Modelles mittels `accuracy_score`.\n",
    "7. Berechnen Sie die Confusion Matrix mittels `confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Aufgabe 1.3 - Was wurde gelernt?\n",
    "\n",
    "Wir können die gelernten Parameter (Gewichte) vom `Neuralen Netz` grafisch betrachten, um eine Intuition zu entwickeln, was das `Neurale Netz` lernt.\n",
    "\n",
    "Hier lesen wir mittels `model.get_weights()[0]` die Parameter vom Modell aus und stellen sie mittels der mitgelieferten Funktion `plot_weights` grafisch dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "plot_weights(model.get_weights()[0], lambda i: f'y{i}', fig=fig)\n",
    "plt.title(\"Gelernte Gewichte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Bild `y0` ist hier das Output Neuron der Ziffer `0` ($\\hat y_0$) und entspricht den hier rot dargestellten Parameter ($\\theta^{(1)}_{0,1}$, ... $\\theta^{(1)}_{0,784}$).\n",
    "\n",
    "![Logistische Regression: Gewichte](./img/logistic-regression-as-nn-mnist-weights.png)\n",
    "\n",
    "`plot_weights` zeigt die Parameter (Verbindungen) von allen 784 Pixeln ($x_1$, ..., $x_{784}$) zu jedem Output Neuron ($\\hat y_0$, ..., $\\hat y_9$).\n",
    "Wir haben 10 Output Neuronen, also 10 Bilder mit 784 Parameters. Die 784 Parameter werden als 28*28 Bild dargestellt.\n",
    "\n",
    "Man kann sich die gelernten Gewichte als eine Maske vorstellen, die wir über das ursprüngliche Bild legen und anschliessend die Werte aufsummieren.\n",
    "Ein positiver Wert der Maske bedeutet, dass eine Aktivierung dieses Pixel **für** die `Klasse` des Outputs spricht.\n",
    "Ein negativer Wert der Maske bedeutet, dass eine Aktivierung dieses Pixel **gegen** die `Klasse` des Outputs spricht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Interpretieren Sie die dargestellten Gewichte vom Bild `y0`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlusswort Aufgabe 1\n",
    "\n",
    "In der Aufgabe 1 haben wir eine `Logistische Regression` als `Neurales Netzwerk` \"nachgebaut\", trainiert und evaluiert.\n",
    "\n",
    "Praktisch macht dies nicht wirklich Sinn, da wir längere Traininszeit hatten, aber keine Verbesserung im Resultat.\n",
    "Das Modell ist ähnlich der `LogisticRegression` im Setup des Aufgabenblattes.\n",
    "\n",
    "Wir haben dafür `tensorflow` kennengelernt und können in Aufgabe 2 unser neues Wissen nutzen und ein `Neurales Netz` mit einem `Hidden Layer` entwickeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "\n",
    "#### Aufgabe 2.1 - (Feed-Forward) Neural Network mit einem Hidden Layer\n",
    "\n",
    "Ziel dieser Aufgabe ist es folgendes Neurales Netzwerk zu bauen:\n",
    "\n",
    "![One-Hidden-Layer (mit 36 Neuronen) Neural Network für MNIST](./img/nn-one-hidden-mnist.png)\n",
    "\n",
    "1. Erstellen Sie das dargestellte Neurale Netzwerk. Achten Sie darauf, dass der `Hidden Layer` eine `Aktivierungsfunktion` mittels dem Parameter `activation` benötigt. Verwenden Sie `tf.keras.activations.relu` oder `tf.keras.activations.gelu`. Der Hidden Layer hat 36 Neuronen `units=36`. In der Praxis nimmt man eher eine Zweierpotzent wie 32, 64 oder 128 (aus Performanzgründen).\n",
    "2. Geben Sie eine Beschreibung des Modell mittels `print(model.summary())` aus. Das Modell sollte **28'630 totale Parameter** haben.\n",
    "3. Kompilieren Sie das Modell analog zur Aufgabe 1.\n",
    "4. Trainieren Sie das Modell auf dem `Train-Set`.\n",
    "5. Evaluieren Sie das Modell auf dem `Validation-Set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Aufgabe 2.2 - Was wurde gelernt?\n",
    "\n",
    "Nun versuchen wir wieder zu verstehen, was für Gewichte gelernt wurden.\n",
    "\n",
    "#### (Extra) Aufgabe 2.2.1 - Input Layer zu Hidden Layer\n",
    "\n",
    "Wir schauen uns zuerst die Gewichte zwischen dem `Input Layer` und dem `Hidden Layer` an.\n",
    "Beispielsweise für die Verbindungen vom Input Layer ($x_1$, ..., $x_{784}$) zum Neuron $z_1$ haben wir die Gewichte $\\theta^{(1)}_{1,1}$, ..., $\\theta^{(1)}_{1,784}$ (hier rot dargestellt).\n",
    "\n",
    "![](./img/nn-one-hidden-mnist-weights-1.png)\n",
    "\n",
    "Mit der Funktion `plot_weights` sind hier die Gewichte von den ersten Verbindungen (`model.get_weights()[0]`) vom Input Layer zum Hidden Layer dargestellt analog zur Aufgabe 1.3.\n",
    "\n",
    "1. Warum sind es jetzt 36 Masken anstatt 10 Masken? \n",
    "2. Können Sie sonst noch etwas erkennnen? Warum erkennt man anders als in Aufgabe 1.3 hier keine klaren Zahlen-Masken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_weights(model.get_weights()[0], lambda i: f'z{i+1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Extra) Aufgabe 2.2.2 - Hidden Layer zu Output Layer\n",
    "\n",
    "Jetzt schauen wir uns die Gewichte zwischen dem Hidden Layer und dem Output Layer an.\n",
    "Diese Gewichte gewichten die 36 Features vom gelernten `Feature Engineering` um vorherzusagen, um welche Ziffer es sich beim ursprünglichen Input handelte. \n",
    "\n",
    "Beispielsweise für die Verbindungen vom Hidden Layer ($z_1$, ..., $z_{36}$) zum Output Neuron $\\hat y_0$ haben wir die Gewichte $\\theta^{(2)}_{0,1}$, ..., $\\theta^{(2)}_{0,36}$ (hier rot dargestellt).\n",
    "\n",
    "![](./img/nn-one-hidden-mnist-weights-2.png)\n",
    "\n",
    "Mit der Funktion `plot_weights` sind hier die Gewichte von den Verbindungen (`model.get_weights()[2]`) vom Hidden Layer zum Output Layer dargestellt analog zur Aufgabe 2.2.1.\n",
    "\n",
    "1. Warum sehen Sie so strukturlos aus? Was bedeuten diese Gewichte? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.get_weights()[2].shape)\n",
    "plot_weights(model.get_weights()[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Extra) Aufgabe 2.2.3 - Input Layer zu Output Layer\n",
    "\n",
    "Bis jetzt haben wir die beiden Layer getrennt betrachtet. \n",
    "\n",
    "Jetzt möchten wir die beiden Layers gemeinsam betrachten, indem wir die 36 gelernten Features anhand der `Input Layer` `Hidden Layer` Gewichte beschreiben, und schauen für ein Input Bild welches dieser Feature wie stark für und gegen eine bestimmte Klasse spricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def get_layer_activations_for(model, input_img):\n",
    "    \"\"\"\n",
    "    Hilfsfunktion für die Berechnung der Aktivierungen von allen Layern in unserem Neuralen Netz.\n",
    "    \"\"\"\n",
    "    input_data = input_img.reshape(1, -1)\n",
    "    first_layer = K.function([model.get_layer(index=0).input], model.get_layer(index=0).output)\n",
    "    second_layer = K.function([model.get_layer(index=0).input], model.get_layer(index=1).output)\n",
    "\n",
    "    return input_img, first_layer([input_data]).reshape(-1), second_layer([input_data]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dazu wird hier folgendes gemacht.\n",
    "Wir betrachten die Neuronen im Hidden Layer ($z_1$, ..., $z_36$) als Gewichtsmasken ($\\theta^{(1)}_{1,1}$, ..., $\\theta^{(1)}_{1,784})$ analog zur Aufgabe 2.2.1. \n",
    "Zuerst schauen wir uns ein Input Image (Bild 1) an (definiert durch `image_index`).\n",
    "Dann schauen wir uns zuerst die Aktivierungen dieser \"Features\" ($z_1$, ..., $z_36$) an (Bild 2). Stark aktive Features werden opak dargestellt, schwach aktive Features werden transparent dargestellt.\n",
    "Anschliessend gewichten wir die Features nach dem Gewicht zu einem bestimmten Ouput (z.B. $\\hat y_0$) - Gewichte dargestellt in Aufgabe 2.2.2.\n",
    "Negative Werte bedeuten dieses \"Feature\" spricht gegen die Ziffer (rot). Positive Werte bedeuten dieses \"Feature\" spricht für die Ziffer (grün).\n",
    "Wie stark ein Feature aktiv ist wird wieder über die Transparenz angezeigt.\n",
    "\n",
    "1. Studieren Sie die Plots unten.\n",
    "2. Was erkennen Sie in den Plots? Wie unterscheiden sich die falschen Klassen von der richtigen Klasse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welches Bild wir nehmen, kann auf 0, 1, 2, ... gesetzt werden, um andere Bilder zu plotten / analysieren.\n",
    "image_index = 2\n",
    "\n",
    "# Berechnung der Aktivierungen innerhalb des Neuralen Netzes für spätere plots.\n",
    "input_layer_output, hidden_layer_ouput, output_layer_ouput = get_layer_activations_for(model, X_val[image_index, :])\n",
    "\n",
    "# Plotten vom Input Image\n",
    "plt.title(\"Input image\")\n",
    "plt.imshow(input_layer_output.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Plotten von der Aktivierung vom Hidden Layer\n",
    "# Welche gelernten Features sind für diese Input-Bild aktiv.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig = plot_weights(model.get_weights()[0], lambda i: f\"z{i}\", alphas=np.abs(hidden_layer_ouput) / np.max(np.abs(hidden_layer_ouput)), fig=fig)\n",
    "fig.suptitle(f\"Welche Features sind für das Bild aktiv?\")\n",
    "plt.show()\n",
    "\n",
    "# Für jedes Output Neuron (0-9) plotten wir die 36 gelernten Features absteigend.\n",
    "# Positive Aktivierungen sprechen für diesen Output (z.B. für die Ziffer 0).\n",
    "# Negative Aktivierungen sprechen gegen diesen Output (z.B. gegen die Ziffer 0).\n",
    "# Die Features wurden transparent gemacht, wenn sie keine grosse Rolle spielen.\n",
    "for i in range(0, 10):\n",
    "    hidden_layer_ouput_weighted = hidden_layer_ouput * model.get_weights()[2][:, i]\n",
    "    colors = np.array(list(map(lambda v: 'viridis' if v >= 0 else 'magma', hidden_layer_ouput_weighted)))\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    fig = plot_weights(model.get_weights()[0], lambda i: f\"z{i}\", alphas=np.abs(hidden_layer_ouput_weighted) / np.max(np.abs(hidden_layer_ouput_weighted)), colors=colors, fig=fig)\n",
    "    fig.suptitle(f\"Welche Features sprechen wie stark (alpha) für (grün) und gegen (rot) die Klasse {i} ({output_layer_ouput[i]: .2f})\")\n",
    "    plt.show()\n",
    "\n",
    "# Plotten von der Aktivierung vom Output Layer (welche Ziffer predicten wir)\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Output Activation\")\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.imshow(output_layer_ouput.reshape(10, 1), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## (Extra) Aufgabe 3 - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Bildern werden im Deep Learning oft `Convolutional Neural Networks` (`CNN`) eingesetzt.\n",
    "\n",
    "CNNs sind laut Drehbuch nicht Teil vom Inhalt von diesem Modul, für interessierte zeigt diese Aufgabe wie man sie einsetzt.  \n",
    "\n",
    "Anders als beim Feed-Forward Neural Network (Aufgabe 2) legen wir nicht Masken über den gesamten Input sondern legen eine kleine Maske (z.B. 3*3 Pixel) und sliden diese über das gesamte Bild. Die Output-Werte dieses Verfahrens sind dann die Aktivierungen (Features) dieser Maske an den verschiedenen Stellen im Bild. \n",
    "\n",
    "Mehr zu CNNs finden Sie hier: https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939\n",
    "\n",
    "1. Machen Sie ein Modell mit `tf.keras.layers.Conv2D` Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Schlusswort Aufgabenblatt 4\n",
    "\n",
    "Deep Learning ist ein sehr grosses Teilgebiet von Machine Learning.\n",
    "\n",
    "Grundsätzlich funktionieren Neurale Netze gut, wenn man **viele Daten** hat und `Feature Engineering` schwierig ist:\n",
    " * Classification auf Bilder\n",
    " * Sprach-Erkennung (Natural Language Processing)\n",
    " * Reinforcement-Learning\n",
    "\n",
    "Das Netzwerk lernt (anhand von Unmengen an Daten) während dem Training Strukturen, die Helfen, die `Kostenfunktion` zu minimieren.\n",
    "Man kann dies als eine Art automatisches `Feature Engineering` sehen - mit genügend und sauberen Daten meist besser als es ein Mensch je hätte machen können.\n",
    "\n",
    "### MNIST\n",
    "\n",
    "Der MNIST Datensatz ist ein sehr bekannter Datensatz. \n",
    "Hier gibt eine Referenz die verschiedene Lösungsansätze und deren Performanz aufzeigt: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "### Ein Modell-Framework\n",
    "\n",
    "`Neurale Netze` kann man auch als **ein Modell-Framework** betrachten:\n",
    "\n",
    "* Es ist einfach das Modell mächtiger zu machen (mehr Neuronen, mehr Layers), wenn man genügend Daten hat um nicht zu `overfitten`.\n",
    "* Die Anordnung der Neuronen (Architektur des Neuralen Netzes) kann dem Netzwerk beim Lernen helfen.\n",
    "  * Feed Forward Neural Network haben wir im Theory Teil kennengelernt (Aufgabe 2), aber es gibt viele weitere: RNN, CNN (Aufgabe 3), Residual Connections, Transformers, Inception, Multi-Task Learning etc.)\n",
    "  * Architekturen treffen meistens Annahmen über das zugrundeliegende Problem und haben daher verschiedene Vor- und Nachteile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}