{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Aufgabenblatt 4\n",
    "\n",
    "`sklearn` hat auch ein Neural Network Modell (https://scikit-learn.org/stable/modules/neural_networks_supervised.html), in der Praxis werden aber oft ausgereiftere `Deep Learning` Libraries verwendet, wie `tensorflow` (Google) oder `PyTorch` (Facebook).\n",
    "\n",
    "Darum verwenden wir für diese Aufgabe `tensorflow` und nicht `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# random seed fixieren für Musterlösung \n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir Dinge von den vorherigen Aufgabenblätter nicht zu fest wiederholen, haben wir hier bereits den `Datensatz` und eine `Baseline` vorbereitet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz - MNIST\n",
    "\n",
    "Wir können den Datensatz über `sklearn.datasets.fetch_openml` laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "y = y.astype(np.int32) # Cast string like '1' to integer like 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir skalieren die Pixelwerte zwischen 0 und 1 (anstatt 0 und 255). Dies ist wichtig für das `Gradient Descent` Lernverfahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Vorher:\", X.max())\n",
    "X = X / 255  # Sehr einfaches skalieren (oft aussreichend für Bilder)\n",
    "print(\"Nach einfachem Skalieren:\", X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend teilen wir die Daten wieder in `Train-Set`, `Validation-Set` und `Test-Set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_data, X_test, y_data, y_test = train_test_split(X, y, test_size=5_000, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=5_000, stratify=y_data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und wir schauen uns Beispiele dieser handgeschriebenen Zahlen vom `Data-Set` an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(10., 10.))\n",
    "\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "    nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "    axes_pad=0.1,  # pad between axes in inch.\n",
    ")\n",
    "\n",
    "for ax, im in zip(grid, X_data.reshape(-1, 28, 28)):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(im, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie es bei der `Classification` eigentlich immer Sinn gibt, schauen wir uns die Verteilung der Zielvariable an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und sehen, dass wir in etwa gleich viele Datenpunkt pro Klasse haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Als `Baseline` haben wir hier bereits eine `Logistische Regression` analog zum Aufgabenblatt 3 trainiert und evaluiert.\n",
    "\n",
    "Wir nehmen allen Pixeln unverändert als Features und fitten das Modell darauf.\n",
    "\n",
    "Dieses `Baseline`-Modell erreicht eine Genauigkeit von ungefähr 92% - ein bereits ziemlich guter Wert, den wir aber in Aufgabe 2 verbessern werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_val_hat = clf.predict(X_val)\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie im Aufgabenblatt 3 können wir für diese `Baseline` die `Confusion Matrix` betrachten.\n",
    "\n",
    "Wir sehen, dass es oft zu Verwechslungen kommt zwischen `7` und `9` oder `5` und `8`. Manche Verwechslungen treten nie auf wie zwischen `0` und `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(y_val_hat, y_val, labels=clf.classes_),\n",
    "        columns=[clf.classes_], # Name columns\n",
    "        index=[clf.classes_] # Name rows\n",
    "    ),\n",
    "    annot=True # Show numbers in heatmap (not just colors)\n",
    ")\n",
    "plt.ylabel('True label') # Name y-axis\n",
    "plt.xlabel('Predicted label') # Name x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Folgende Funktion müssen Sie in den Extra-Aufgaben verwenden. Die Umsetzung (der Code) der Funktion muss **nicht** verstanden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt, floor, ceil\n",
    "\n",
    "def plot_weights(weights: np.array, f = None, alphas=None, colors=None, fig=None):\n",
    "\n",
    "    if alphas is None:\n",
    "        alphas = itertools.cycle([1]) \n",
    "    \n",
    "    if colors is None:\n",
    "        colors = itertools.cycle(['viridis'])\n",
    "    \n",
    "    if f is None:\n",
    "        f = lambda x: x\n",
    "        \n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=(18, 18))\n",
    "    \n",
    "    def get_2d_dim_of(size):\n",
    "        n_rows = floor(sqrt(size))\n",
    "        n_cols = ceil(sqrt(size))\n",
    "        if n_rows * n_cols < size:\n",
    "            n_rows = n_rows + 1\n",
    "        return n_rows, n_cols\n",
    "\n",
    "    size = weights.shape[1]\n",
    "    n_rows = floor(sqrt(size))\n",
    "    n_cols = ceil(sqrt(size))\n",
    "    size = weights.shape[0]\n",
    "    img_n_rows = floor(sqrt(size))\n",
    "    img_n_cols = ceil(sqrt(size))\n",
    "    if n_rows * n_cols < size:\n",
    "        n_rows = n_rows + 1\n",
    "\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "        nrows_ncols=get_2d_dim_of(weights.shape[1]),  # creates 2x2 grid of axes\n",
    "        axes_pad=(0.1, 0.3),\n",
    "        share_all=True,\n",
    "        cbar_location='right',\n",
    "        cbar_mode='single',\n",
    "        direction = 'row',\n",
    "        cbar_size='10%',\n",
    "        cbar_pad=0.1,\n",
    "    )\n",
    "\n",
    "    vmin, vmax = weights.min(), weights.max()\n",
    "    for i, (ax, cax, w, color, alpha) in enumerate(zip(grid, grid.cbar_axes, np.rollaxis(weights, 1), colors, alphas)):\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f(i))\n",
    "        im = ax.imshow(w.reshape(*get_2d_dim_of(weights.shape[0])), alpha=alpha, vmin=vmin, vmax=vmax, cmap=color)\n",
    "        # Plot colorbar for im (due to fixed vmin and vmax same for all images)\n",
    "        cb = plt.colorbar(im, cax=cax)\n",
    "        cb.set_alpha(1)\n",
    "        cb.draw_all()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aufgabe 1 - Logistische Regression als Neural Network\n",
    "\n",
    "In Aufgabe 1 bauen wir die `Logistische Regression` (die `Baseline` aus der Aufgabenbeschreibung) als `Neurales Netzwerk` nach. Anschliessend trainieren und evaluieren wir das Netzwerk.\n",
    "\n",
    "Dieses `Neurales Netzwerk` hat noch keinen `Hidden Layer` und ist dadurch ein lineares Modell. Es entspricht (bis auf wie es trainiert wird) der `Logistischen Regression` von `sklearn` und wir erwarten daher in Aufgabe 1 (noch) keine Verbesserung zur `Baseline`.\n",
    "\n",
    "In der Aufgabe 2 schauen wir uns dann ein `Neurales Netz` mit einem `Hidden Layer` mit nicht linearer Aktivierungsfunktion an - ein nicht lineares Modell.\n",
    "\n",
    "### Aufgabe 1.1 - Logistische Regression als (Feed Forward) Neural Network\n",
    "\n",
    "1. Erstellen Sie ein sequenzielles Neurales Netzwerk mittels `tf.keras.Sequential`\n",
    "2. Fügen Sie einen Input Layer `tf.keras.layers.InputLayer` mit der shape `input_shape=(28*28,)` hinzu mittels `model.add`.\n",
    "3. Fügen Sie einen Output Layer `tf.keras.layers.Dense` mit `units=10` und der Softmax Aktivierungsfunktion `activation=tf.keras.activations.softmax` hinzu mittels `model.add`.\n",
    "4. Geben Sie eine Beschreibung des Modell mittels `print(model.summary())` aus. Das Modell sollte 7850 totale Parameter haben.\n",
    "5. (Extra) Rechnen Sie die Anzahl Parameter von Hand nach.\n",
    "\n",
    "Zur Illustration ist das Neurale Netzwerk von Aufgabe 1 hier grafisch abgebildet. Es hat 784 Inputs ($x_1$, ..., $x_{784}$) für jeden Pixel ein Input (28 * 28 = 784) und 10 Ouputs ($\\hat y_0$, ..., $\\hat y_9$) für jede `Klasse` (Ziffer) einen Output.\n",
    "\n",
    "![Logistische Regression als Neural Network](./img/logistic-regression-as-nn-mnist.png)\n",
    "\n",
    "\n",
    "#### Hilfreiche Links\n",
    "\n",
    "* tf.keras.Sequential: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Neural Network ohne Hidden Layer\n",
    "# 1.\n",
    "model = tf.keras.Sequential()\n",
    "# 2. \n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(28*28,), name='input_layer'))\n",
    "# 3.\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax, name='output_layer'))\n",
    "\n",
    "# 4.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\.\n",
    "Wir haben für jeden Input ($x_1$ bis $x_{784}$) und für den Bias ($x_0$) jeweils einen Verbindung (Pfeil) zu jedem Output ($\\hat y_0$ bis $\\hat y_9$).\n",
    "Für jede Verbindung haben wird ein Parameter (Gewicht): \n",
    "$\\text{Anzahl Verbindungen} = (\\text{Inputs} + \\text{Bias}) * \\text{Ouputs} = (784 + 1) * 10 = 7850 = \\text{Total Parameter} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.2 - `model.compile`, `model.fit`\n",
    "\n",
    "In Aufgabe 1.1 haben wir das Netzwerk erstellt und mit `model.summary()` überprüft.\n",
    "\n",
    "Nun möchten wir das Modell trainieren (`fit`).\n",
    "\n",
    "Dazu müssen wir dem Modell mitteilen, welches Optimierungsverfahren (`optimizer`) und welche Kostenfunktion (`loss`) wir für das Training verwenden möchten.\n",
    "\n",
    "In der `Logistischen Regression` von `sklearn` sind diese Werte im Modell hard-codiert vorgegeben und man kann sie nicht überschreiben. \n",
    "`Neurale Netze` von `tensorflow` können mit unterschiedlichen Kostenfunktionen (`tf.keras.losses`) und unterschiedlichen Optimiertungsverfahren (z.B. `adam`) trainiert werden. Darum muss man diese Werte für ein `tensorflow` Modell noch angeben.\n",
    "\n",
    "1. Erstellen Sie eine `tf.keras.losses.SparseCategoricalCrossentropy` Funktion mit `from_logits=False`.\n",
    "2. Kompilieren Sie das Modell mit `model.compile`. Geben Sie die `loss` Funktion und den `optimizer` an. Für den `optimizer` können Sie `sgd` für das `Batch Gradient Descent` Verfahren oder `adam` für `Adam` ein verbessertes `Batch Gradient Descent` Verfahren.\n",
    "3. Trainieren Sie das Modell mittels `model.fit`. Wir müssen eine Batch Size `batch_size=64` und die Anzahl Epochen `epochs=15` mitgeben:\n",
    "    - `batch_size`: Wie viele Datenpunkte werden für einen Schritt im `Batch Gradient Descent` Verfahren verwendet.\n",
    "    - `epochs`: Wie oft iterieren wir über das gesamte `Train-Set`.\n",
    "4. Berechnen Sie die Wahrscheinlichkeiten der Klassen `y_val_hat_prob` auf dem `Validation-Set` (`X_val`) mittels `model.predict`.\n",
    "    - Anders als die Logistische Regression gibt uns `model.predict` hier 10 Outputs, **die Wahrscheinlichkeiten** für jede `Klasse`.\n",
    "5. Holen Sie die tatsächliche Vorhersagen `y_val_hat` aus den Wahrscheinlichkeiten `y_val_hat_prob` aus Schritt 2. Die tatsächliche Vorhersage ist der Output mit dem grössten Wert (der grösste Wahrscheinlichkeit).\n",
    "    - Nutzen Sie dazu `np.argmax` über die zweite Achse `axis=1`.\n",
    "    - Geben Sie die `shape` von `y_val_hat_prob` und von `y_val_hat` aus, um besser zu verstehen, was wir genau machen.\n",
    "6. Berechnen Sie die Genauigkeit unseres Modelles mittels `accuracy_score`.\n",
    "7. Berechnen Sie die Confusion Matrix mittels `confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# 2.\n",
    "model.compile(\n",
    "    optimizer='adam', # adam ist eine Form von Stochastic Gradient Descent mit extra Schritten, damit das Netzwerk schneller lernt.\n",
    "    loss=loss_fn,    \n",
    "    metrics=['accuracy'] # Metrik können für das Ausgeben von zwischen Ergebnissen angegeben werden\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Wir speichern noch die `fit`-history, um sie in anschliessend zu plotten.\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plotten der History: Zeigt den Verlauf des Trainings über die Anzahl Epochen. (Nicht Teil der Aufgabe)\n",
    "# Hierfür muss bei `model.compile` di `accuracy` Metrik und bei fit das `Validation-Set` mitgegeben worden sein.\n",
    "plt.title('Accuracy Curve')\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.\n",
    "y_val_hat_prob = model.predict(X_val)\n",
    "# 5.\n",
    "y_val_hat = np.argmax(y_val_hat_prob, axis=1)\n",
    "\n",
    "print(\"y_val_hat_prob.shape: \", y_val_hat_prob.shape)\n",
    "print(\"y_val_hat.shape: \", y_val_hat.shape)\n",
    "\n",
    "# 6.\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `Accuracy` unseres `Neuralen Netzwerkes` **ohne** `Hidden Layer` entspricht in etwa der Performanz der `Logistischen Regression`, was, wie oben beschrieben, zu erwarten war. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 7.\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(y_val_hat, y_val, labels=clf.classes_),\n",
    "        columns=[clf.classes_], # Name columns\n",
    "        index=[clf.classes_] # Name rows\n",
    "    ),\n",
    "    annot=True # Show numbers in heatmap (not just colors)\n",
    ")\n",
    "plt.ylabel('True label') # Name y-axis\n",
    "plt.xlabel('Predicted label') # Name x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Aufgabe 1.3 - Was wurde gelernt?\n",
    "\n",
    "Wir können die gelernten Parameter (Gewichte) vom `Neuralen Netz` grafisch betrachten, um eine Intuition zu entwickeln, was das `Neurale Netz` lernt.\n",
    "\n",
    "Hier lesen wir mittels `model.get_weights()[0]` die Parameter vom Modell aus und stellen sie mittels der mitgelieferten Funktion `plot_weights` grafisch dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "plot_weights(model.get_weights()[0], lambda i: f'y{i}', fig=fig)\n",
    "plt.title(\"Gelernte Gewichte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Bild `y0` ist hier das Output Neuron der Ziffer `0` ($\\hat y_0$) und entspricht den hier rot dargestellten Parameter ($\\theta^{(1)}_{0,1}$, ... $\\theta^{(1)}_{0,784}$).\n",
    "\n",
    "![Logistische Regression: Gewichte](./img/logistic-regression-as-nn-mnist-weights.png)\n",
    "\n",
    "`plot_weights` zeigt die Parameter (Verbindungen) von allen 784 Pixeln ($x_1$, ..., $x_{784}$) zu jedem Output Neuron ($\\hat y_0$, ..., $\\hat y_9$).\n",
    "Wir haben 10 Output Neuronen, also 10 Bilder mit 784 Parameters. Die 784 Parameter werden als 28*28 Bild dargestellt.\n",
    "\n",
    "Man kann sich die gelernten Gewichte als eine Maske vorstellen, die wir über das ursprüngliche Bild legen und anschliessend die Werte aufsummieren.\n",
    "Ein positiver Wert der Maske bedeutet, dass eine Aktivierung dieses Pixel **für** die `Klasse` des Outputs spricht.\n",
    "Ein negativer Wert der Maske bedeutet, dass eine Aktivierung dieses Pixel **gegen** die `Klasse` des Outputs spricht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Interpretieren Sie die dargestellten Gewichte vom Bild `y0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1\\.\n",
    "\n",
    "Die Maske `y0` hat negative Gewichte in der Mitte des Bildes. Aktive Pixel in der Mitte des Input-Bild sprechen also **gegen** die `Klasse` `0`.\n",
    "Pixel um die Mitte des Input-Bild (in einer `0` Form :-)) sprechen für die `Klasse` `0`.\n",
    "\n",
    "Beachten Sie, dass diese Masken jeden Pixel (Input) **unabhängig von allen anderen Pixel gewichten**.\n",
    "Es kann zu **keiner Interaktion zwischen den Pixel** (zwischen den Inputs) kommen!\n",
    "Anders ausgedrückt: Der Wert eines Outputs ist eine gewichtete Summe aller Pixel (aller Inputs).\n",
    "Eine gewichtete Summe der Inputs ist nichts anderes als ein Lineares Modell.\n",
    "\n",
    "In Aufgabe 2 werden wir ein Hidden Layer hinzufügen und somit **Interaktionen zwischen Pixel (zwischen Inputs) möglich machen**.\n",
    "Das Modell wird damit nicht-linear und ist flexibler.\n",
    "Da wir genügend Daten haben, um die Flexibilität des Modelles intelligent zu nutzen, sprich wir nicht `Overfitten`, ist das Modell aus Aufgabe 2 dann auch besser als dieses Modell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlusswort Aufgabe 1\n",
    "\n",
    "In der Aufgabe 1 haben wir eine `Logistische Regression` als `Neurales Netzwerk` \"nachgebaut\", trainiert und evaluiert.\n",
    "\n",
    "Praktisch macht dies nicht wirklich Sinn, da wir längere Traininszeit hatten, aber keine Verbesserung im Resultat.\n",
    "Das Modell ist ähnlich der `LogisticRegression` im Setup des Aufgabenblattes.\n",
    "\n",
    "Wir haben dafür `tensorflow` kennengelernt und können in Aufgabe 2 unser neues Wissen nutzen und ein `Neurales Netz` mit einem `Hidden Layer` entwickeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "\n",
    "#### Aufgabe 2.1 - (Feed-Forward) Neural Network mit einem Hidden Layer\n",
    "\n",
    "Ziel dieser Aufgabe ist es folgendes Neurales Netzwerk zu bauen:\n",
    "\n",
    "![One-Hidden-Layer (mit 36 Neuronen) Neural Network für MNIST](./img/nn-one-hidden-mnist.png)\n",
    "\n",
    "1. Erstellen Sie das dargestellte Neurale Netzwerk. Achten Sie darauf, dass der `Hidden Layer` eine `Aktivierungsfunktion` mittels dem Parameter `activation` benötigt. Verwenden Sie `tf.keras.activations.relu` oder `tf.keras.activations.gelu`. Der Hidden Layer hat 36 Neuronen `units=36`. In der Praxis nimmt man eher eine Zweierpotzent wie 32, 64 oder 128 (aus Performanzgründen).\n",
    "2. Geben Sie eine Beschreibung des Modell mittels `print(model.summary())` aus. Das Modell sollte **28'630 totale Parameter** haben.\n",
    "3. Kompilieren Sie das Modell analog zur Aufgabe 1.\n",
    "4. Trainieren Sie das Modell auf dem `Train-Set`.\n",
    "5. Evaluieren Sie das Modell auf dem `Validation-Set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Neural Network mit einem Hidden Layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28*28,), name='input_layer'),\n",
    "    tf.keras.layers.Dense(36, activation=tf.keras.activations.gelu, name='hidden_layer'),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax, name='output_layer'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "# 2. \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 5.\n",
    "y_val_hat_prob = model.predict(X_val)\n",
    "y_val_hat = np.argmax(y_val_hat_prob, axis=1)\n",
    "\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es kam mit einer Genauigkeit von 95% zu einer Verbesserung gegenüber Aufgabe 1, wo wir eine Genauigkeit von 92% erzielten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(y_val_hat, y_val, labels=clf.classes_),\n",
    "        columns=[clf.classes_], # Name columns\n",
    "        index=[clf.classes_] # Name rows\n",
    "    ),\n",
    "    annot=True # Show numbers in heatmap (not just colors)\n",
    ")\n",
    "plt.ylabel('True label') # Name y-axis\n",
    "plt.xlabel('Predicted label') # Name x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Aufgabe 2.2 - Was wurde gelernt?\n",
    "\n",
    "Nun versuchen wir wieder zu verstehen, was für Gewichte gelernt wurden.\n",
    "\n",
    "#### (Extra) Aufgabe 2.2.1 - Input Layer zu Hidden Layer\n",
    "\n",
    "Wir schauen uns zuerst die Gewichte zwischen dem `Input Layer` und dem `Hidden Layer` an.\n",
    "Beispielsweise für die Verbindungen vom Input Layer ($x_1$, ..., $x_{784}$) zum Neuron $z_1$ haben wir die Gewichte $\\theta^{(1)}_{1,1}$, ..., $\\theta^{(1)}_{1,784}$ (hier rot dargestellt).\n",
    "\n",
    "![](./img/nn-one-hidden-mnist-weights-1.png)\n",
    "\n",
    "Mit der Funktion `plot_weights` sind hier die Gewichte von den ersten Verbindungen (`model.get_weights()[0]`) vom Input Layer zum Hidden Layer dargestellt analog zur Aufgabe 1.3.\n",
    "\n",
    "1. Warum sind es jetzt 36 Masken anstatt 10 Masken? \n",
    "2. Können Sie sonst noch etwas erkennnen? Warum erkennt man anders als in Aufgabe 1.3 hier keine klaren Zahlen-Masken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_weights(model.get_weights()[0], lambda i: f'z{i+1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\.\n",
    "Anders als in Aufgabe 1.3 gehen wir hier nicht direkt zum Output Layer (10 Neuronen) sondern zuerst zum Hidden Layer (36 Neuronen).\n",
    "Also haben wir vom Input Layer (784) zum Hidden Layer (36) 36 mal 784 Verbindungen oder anders gesagt 36 Bilder von 28 * 28 Pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape der Gewichte\", model.get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\.\n",
    "Der Hidden Layer kann als automatisches `Feature Engineering` verstanden werden. Das `Neurale Netz` lernt anhand der Daten in den Input Bildern Strukturen zu erkennen, die es für die Classification **im Output Layer verwenden** kann. Die hier dargestellte Masken sind also `Feature Extractors`, also wir reduzieren die 784 Pixel (Input-Features) zu 36 neue (Latent-)Features. Diese Features sind beispielsweise aktiv, wenn eine gewisse Region im Ursprungsbild aktiv ist.\n",
    "Diese 36 neuen Features werden dann im zweiten Layer (Hidden Layer zu Output Layer) verwendet, um vorherzusagen um welche Ziffer es sich handelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Extra) Aufgabe 2.2.2 - Hidden Layer zu Output Layer\n",
    "\n",
    "Jetzt schauen wir uns die Gewichte zwischen dem Hidden Layer und dem Output Layer an.\n",
    "Diese Gewichte gewichten die 36 Features vom gelernten `Feature Engineering` um vorherzusagen, um welche Ziffer es sich beim ursprünglichen Input handelte. \n",
    "\n",
    "Beispielsweise für die Verbindungen vom Hidden Layer ($z_1$, ..., $z_{36}$) zum Output Neuron $\\hat y_0$ haben wir die Gewichte $\\theta^{(2)}_{0,1}$, ..., $\\theta^{(2)}_{0,36}$ (hier rot dargestellt).\n",
    "\n",
    "![](./img/nn-one-hidden-mnist-weights-2.png)\n",
    "\n",
    "Mit der Funktion `plot_weights` sind hier die Gewichte von den Verbindungen (`model.get_weights()[2]`) vom Hidden Layer zum Output Layer dargestellt analog zur Aufgabe 2.2.1.\n",
    "\n",
    "1. Warum sehen Sie so strukturlos aus? Was bedeuten diese Gewichte? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.get_weights()[2].shape)\n",
    "plot_weights(model.get_weights()[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\.\n",
    "Wir erkennen kein klares Muster in den Gewichten. Die 36 Gewichte bestimmen welches der 36 gelernten Feature im Hidden Layer (vom automatischen `Feature Engineering` beschrieben in der Musterlösung von Aufgabe 2.2.1) wie stark gewichtet werden. Die gelernten Features haben keine bestimmte Reihenfolge, darum haben die Gewichte auch keine klare Struktur.\n",
    "Ein negativer Wert eines Gewichtes bedeutet, dass dieses Feature gegen den Output spricht. Ein positiver Wert eines Gewichtes bedeutet, dass dieses Feature für den Ouput spricht. (Die Features sind wegen der ReLU (oder GeLU) Aktivierungsfunktion immer positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Extra) Aufgabe 2.2.3 - Input Layer zu Output Layer\n",
    "\n",
    "Bis jetzt haben wir die beiden Layer getrennt betrachtet. \n",
    "\n",
    "Jetzt möchten wir die beiden Layers gemeinsam betrachten, indem wir die 36 gelernten Features anhand der `Input Layer` `Hidden Layer` Gewichte beschreiben, und schauen für ein Input Bild welches dieser Feature wie stark für und gegen eine bestimmte Klasse spricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def get_layer_activations_for(model, input_img):\n",
    "    \"\"\"\n",
    "    Hilfsfunktion für die Berechnung der Aktivierungen von allen Layern in unserem Neuralen Netz.\n",
    "    \"\"\"\n",
    "    input_data = input_img.reshape(1, -1)\n",
    "    first_layer = K.function([model.get_layer(index=0).input], model.get_layer(index=0).output)\n",
    "    second_layer = K.function([model.get_layer(index=0).input], model.get_layer(index=1).output)\n",
    "\n",
    "    return input_img, first_layer([input_data]).reshape(-1), second_layer([input_data]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dazu wird hier folgendes gemacht.\n",
    "Wir betrachten die Neuronen im Hidden Layer ($z_1$, ..., $z_36$) als Gewichtsmasken ($\\theta^{(1)}_{1,1}$, ..., $\\theta^{(1)}_{1,784})$ analog zur Aufgabe 2.2.1. \n",
    "Zuerst schauen wir uns ein Input Image (Bild 1) an (definiert durch `image_index`).\n",
    "Dann schauen wir uns zuerst die Aktivierungen dieser \"Features\" ($z_1$, ..., $z_36$) an (Bild 2). Stark aktive Features werden opak dargestellt, schwach aktive Features werden transparent dargestellt.\n",
    "Anschliessend gewichten wir die Features nach dem Gewicht zu einem bestimmten Ouput (z.B. $\\hat y_0$) - Gewichte dargestellt in Aufgabe 2.2.2.\n",
    "Negative Werte bedeuten dieses \"Feature\" spricht gegen die Ziffer (rot). Positive Werte bedeuten dieses \"Feature\" spricht für die Ziffer (grün).\n",
    "Wie stark ein Feature aktiv ist wird wieder über die Transparenz angezeigt.\n",
    "\n",
    "1. Studieren Sie die Plots unten.\n",
    "2. Was erkennen Sie in den Plots? Wie unterscheiden sich die falschen Klassen von der richtigen Klasse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welches Bild wir nehmen, kann auf 0, 1, 2, ... gesetzt werden, um andere Bilder zu plotten / analysieren.\n",
    "image_index = 2\n",
    "\n",
    "# Berechnung der Aktivierungen innerhalb des Neuralen Netzes für spätere plots.\n",
    "input_layer_output, hidden_layer_ouput, output_layer_ouput = get_layer_activations_for(model, X_val[image_index, :])\n",
    "\n",
    "# Plotten vom Input Image\n",
    "plt.title(\"Input image\")\n",
    "plt.imshow(input_layer_output.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Plotten von der Aktivierung vom Hidden Layer\n",
    "# Welche gelernten Features sind für diese Input-Bild aktiv.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig = plot_weights(model.get_weights()[0], lambda i: f\"z{i}\", alphas=np.abs(hidden_layer_ouput) / np.max(np.abs(hidden_layer_ouput)), fig=fig)\n",
    "fig.suptitle(f\"Welche Features sind für das Bild aktiv?\")\n",
    "plt.show()\n",
    "\n",
    "# Für jedes Output Neuron (0-9) plotten wir die 36 gelernten Features absteigend.\n",
    "# Positive Aktivierungen sprechen für diesen Output (z.B. für die Ziffer 0).\n",
    "# Negative Aktivierungen sprechen gegen diesen Output (z.B. gegen die Ziffer 0).\n",
    "# Die Features wurden transparent gemacht, wenn sie keine grosse Rolle spielen.\n",
    "for i in range(0, 10):\n",
    "    hidden_layer_ouput_weighted = hidden_layer_ouput * model.get_weights()[2][:, i]\n",
    "    colors = np.array(list(map(lambda v: 'viridis' if v >= 0 else 'magma', hidden_layer_ouput_weighted)))\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    fig = plot_weights(model.get_weights()[0], lambda i: f\"z{i}\", alphas=np.abs(hidden_layer_ouput_weighted) / np.max(np.abs(hidden_layer_ouput_weighted)), colors=colors, fig=fig)\n",
    "    fig.suptitle(f\"Welche Features sprechen wie stark (alpha) für (grün) und gegen (rot) die Klasse {i} ({output_layer_ouput[i]: .2f})\")\n",
    "    plt.show()\n",
    "\n",
    "# Plotten von der Aktivierung vom Output Layer (welche Ziffer predicten wir)\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Output Activation\")\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.imshow(output_layer_ouput.reshape(10, 1), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\.\n",
    "Die richtige Klasse hat viele positive aktive Features, darum wird auch diese Klasse vorhergesagt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## (Extra) Aufgabe 3 - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Bildern werden im Deep Learning oft `Convolutional Neural Networks` (`CNN`) eingesetzt.\n",
    "\n",
    "CNNs sind laut Drehbuch nicht Teil vom Inhalt von diesem Modul, für interessierte zeigt diese Aufgabe wie man sie einsetzt.  \n",
    "\n",
    "Anders als beim Feed-Forward Neural Network (Aufgabe 2) legen wir nicht Masken über den gesamten Input sondern legen eine kleine Maske (z.B. 3*3 Pixel) und sliden diese über das gesamte Bild. Die Output-Werte dieses Verfahrens sind dann die Aktivierungen (Features) dieser Maske an den verschiedenen Stellen im Bild. \n",
    "\n",
    "Mehr zu CNNs finden Sie hier: https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939\n",
    "\n",
    "1. Machen Sie ein Modell mit `tf.keras.layers.Conv2D` Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network mit zwei Hidden Convolution Layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28, 28, 1), name='input_layer'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2), activation=tf.keras.activations.gelu, name='hidden_layer_1'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2), activation=tf.keras.activations.gelu, name='hidden_layer_2'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax, name='output_layer'),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Without GPU (or TPU :D) support, this may take a while.\n",
    "Convolutions on CPUs (in tensorflow) are quit slow.\n",
    "Note that this model has less total parameters than Aufgabe 2.\n",
    "\"\"\"\n",
    "\n",
    "X_train_sq = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val_sq = X_val.reshape(-1, 28, 28, 1)\n",
    "\n",
    "history = model.fit(X_train_sq, y_train, batch_size=128, epochs=15, validation_data=(X_val_sq, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_val_hat_prob = model.predict(X_val_sq)\n",
    "y_val_hat = np.argmax(y_val_hat_prob, axis=1)\n",
    "\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erreichen eine höhere Genauigkeit als in Aufgabe 2, haben aber weniger Parameter als in Aufgabe 2 im Neuralen Netz. Durch die andere Struktur des Netzwerkes nutzen wir die Parameter besser für den Bild-Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(y_val_hat, y_val, labels=clf.classes_),\n",
    "        columns=[clf.classes_], # Name columns\n",
    "        index=[clf.classes_] # Name rows\n",
    "    ),\n",
    "    annot=True # Show numbers in heatmap (not just colors)\n",
    ")\n",
    "plt.ylabel('True label') # Name y-axis\n",
    "plt.xlabel('Predicted label') # Name x-axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Schlusswort Aufgabenblatt 4\n",
    "\n",
    "Deep Learning ist ein sehr grosses Teilgebiet von Machine Learning.\n",
    "\n",
    "Grundsätzlich funktionieren Neurale Netze gut, wenn man **viele Daten** hat und `Feature Engineering` schwierig ist:\n",
    " * Classification auf Bilder\n",
    " * Sprach-Erkennung (Natural Language Processing)\n",
    " * Reinforcement-Learning\n",
    "\n",
    "Das Netzwerk lernt (anhand von Unmengen an Daten) während dem Training Strukturen, die Helfen, die `Kostenfunktion` zu minimieren.\n",
    "Man kann dies als eine Art automatisches `Feature Engineering` sehen - mit genügend und sauberen Daten meist besser als es ein Mensch je hätte machen können.\n",
    "\n",
    "### MNIST\n",
    "\n",
    "Der MNIST Datensatz ist ein sehr bekannter Datensatz. \n",
    "Hier gibt eine Referenz die verschiedene Lösungsansätze und deren Performanz aufzeigt: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "### Ein Modell-Framework\n",
    "\n",
    "`Neurale Netze` kann man auch als **ein Modell-Framework** betrachten:\n",
    "\n",
    "* Es ist einfach das Modell mächtiger zu machen (mehr Neuronen, mehr Layers), wenn man genügend Daten hat um nicht zu `overfitten`.\n",
    "* Die Anordnung der Neuronen (Architektur des Neuralen Netzes) kann dem Netzwerk beim Lernen helfen.\n",
    "  * Feed Forward Neural Network haben wir im Theory Teil kennengelernt (Aufgabe 2), aber es gibt viele weitere: RNN, CNN (Aufgabe 3), Residual Connections, Transformers, Inception, Multi-Task Learning etc.)\n",
    "  * Architekturen treffen meistens Annahmen über das zugrundeliegende Problem und haben daher verschiedene Vor- und Nachteile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}