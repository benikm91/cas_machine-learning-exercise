{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) # Zufall fixieren für Musterlösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Aufgabenblatt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "\n",
    "In diesem Aufgabenblatt verwenden wir den Iris-Datensatz.\n",
    "\n",
    "Der Iris-Datensatz wird in vielen Beispielen und Büchern über Classification in Machine Learning verwendet.\n",
    "Sie finden also online zusätzliche Informationen und Lösungs-Ansätze zu diesem Datensatz.\n",
    "Der Datensatz ist sogar in `sklearn` integriert, sprich wir können den Iris-Datensatz direkt über `sklearn.datasets`  laden.\n",
    "\n",
    "Der Iris-Datensatz ist hier beschrieben: https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "| Feature            | Descriptiopn                                                                  |\n",
    "|--------------------|-------------------------------------------------------------------------------|\n",
    "| specie             | Spezie der Blume                                                              |\n",
    "| sepal length (cm)  | Länge des Kelchblattes der Blume (in Zentimeter).                             |\n",
    "| sepal width (cm)   | Breite des Kelchblattes der Blume (in Zentimeter).                            |\n",
    "| petal length (cm)  | Länge des Kronblattes der Blume (in Zentimeter).                              |\n",
    "| petal width (cm)   | Breite des Kelchblattes der Blume (in Zentimeter).                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ziel\n",
    "\n",
    "Das Ziel ist es anhand der Features vorhersagen zu können, um welche Blumenart es sich handelt.\n",
    "Wir sagen also basierend auf Inputs (`Features`) ein Output (`Label`, auch `Klasse`) voraus - wir machen also eine `Classification`.\n",
    "\n",
    "![Ziel des Aufgabenblattes](./img/goal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Im Setup sind bereits notwendige Schritte implementiert, die Sie im Aufgabenblatt 2 selbst implementierten.\n",
    "**Nach Aufgabenblatt 2 sollten die Schritte klar sein!**\n",
    "\n",
    "Wir laden hier die Daten über `sklearn.datasets`, und teilen anschliessend die Features in `X` und Zielvariable in `y` auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Laden des Iris-Datensatzes über sklearn.datasets\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "# Aufteilen der Daten in Features und Zielvariable\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# iris.target ist als Nummer abgespeichert (0, 1 oder 2), hier schlüsseln wir diese Codierung in die Spezien-Namen auf (target_names).\n",
    "y = y.apply(lambda key: iris.target_names[key]).rename('specie')\n",
    "\n",
    "display(HTML('X:'))\n",
    "display(X.head())\n",
    "display(HTML('y:'))\n",
    "display(y.to_frame().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Analog zum Aufgabenblatt 2 machen wir hier ein `Test-Set`, `Validation-Set` und `Train-Set`, welche wir in den Aufgaben verwenden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split in Test-Set and Data-Set\n",
    "X_data, X_test, y_data, y_test = train_test_split(X, y, random_state=2, stratify=y)\n",
    "# Split Data-Set in Train-Set and Validation-Set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, random_state=2, stratify=y_data)\n",
    "\n",
    "print(\"Test-Set\", X_test.shape)\n",
    "print(\"Data-Set\", X_data.shape)\n",
    "print(\"Train-Set\", X_train.shape)\n",
    "print(\"Validation-Set\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für `seaborn` in der Datenanalyse (Aufgabe 1) setzen wir `X_data` und `y_data` zu einem DataFrame `df_data` zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = pd.concat([X_data, y_data.to_frame()], axis=1)\n",
    "\n",
    "display(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende Funktion müssen Sie in den Aufgaben verwenden. Die Umsetzung (der Code) der Funktion **muss nicht verstanden** werden. \n",
    "Die Funktion hilft die `Decision Region` eines Classifiers zu plotten. Wir verwenden die Funktion in späteren Aufgaben, um die Classifiers besser zu verstehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model._base import LinearClassifierMixin\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(clf: LinearClassifierMixin, data: pd.DataFrame, x: str, y: str, colors = None, num_steps=1000):\n",
    "    \"\"\"\n",
    "    Funktion zum Plotten der Decision Region eines Classification Modells.\n",
    "    Es wird nicht erwartet, dass Sie die Umsetzung dieser Funktion verstehen.\n",
    "    :param clf: Classification Modell\n",
    "    :param data: Daten als DataFrame\n",
    "    :param x: Spaltenname der X-Achse\n",
    "    :param y: Spaltenname der Y-Achse\n",
    "    :param colors: Farben (optional)\n",
    "    :param num_steps: Auflösung des Decision Region Plots.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    xs = data[x]\n",
    "    ys = data[y]\n",
    "    x_min, x_max = xs.min() - 1, xs.max() + 1\n",
    "    y_min, y_max = ys.min() - 1, ys.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, abs(x_max - x_min) / num_steps), np.arange(y_min, y_max, abs(y_max - y_min) / num_steps))\n",
    "\n",
    "    zz = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    zz = zz.reshape(xx.shape)\n",
    "\n",
    "    if colors is not None:\n",
    "        # Map labels to color index for plt.contourf\n",
    "        zz = np.vectorize(lambda x: list(colors.keys()).index(x))(zz)\n",
    "        cmap = ListedColormap(list(colors.values()))\n",
    "        plt.contourf(xx, yy, zz, alpha=0.25, cmap=cmap, extend='both')\n",
    "    else:\n",
    "        zz = np.vectorize(lambda x: list(clf.classes_).index(x))(zz)\n",
    "        plt.contourf(xx, yy, zz, alpha=0.25, extend='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farben werden in der Musterlösung wiederverwendet um ein einheitliches Farbschema über die Aufgaben hinweg zu haben.\n",
    "# Für die Lösung der Aufgaben sind sie nicht relevant.\n",
    "colors = {\n",
    "    'versicolor': 'red',\n",
    "    'virginica': 'blue',\n",
    "    'setosa': 'green'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 - Daten Analyse\n",
    "\n",
    "Wir möchten Anhand unserer Features (wie `sepal length (cm)`) die Blumenspezie (`specie`) vorhersagen können.\n",
    "\n",
    "In Aufgabe 2 werden wir dafür ein Modell erstellen. In Aufgabe 1 werden wir erst einmal die Daten mit ein paar Plots genauer anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Aufgabe 1.1 - Histogram der Zielvariable\n",
    "\n",
    "Es macht immer Sinn, sich die **Verteilung der Zielvariable** anzuschauen.\n",
    "Bei einer `Classification` kann man einfach die Anzahl Datenpunkte pro `Klasse` ausgeben.\n",
    "\n",
    "1. Erstellen Sie ein Plot der Zielvariable `specie` mittels `sns.countplot` (oder `sns.histplot`)\n",
    "2. Interpretieren Sie den Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. mittels histplot\n",
    "sns.histplot(data=df_data, x='specie')\n",
    "plt.close() # Do not show plot\n",
    "\n",
    "# 1. mittels countplot (palette=colors ist nicht Teil der Aufgabenstellung).\n",
    "sns.countplot(data=df_data, x='specie', palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2\\.\n",
    "Wir haben 3 `Klassen` und von allen `Klassen` über 35 Datenpunkte.\n",
    "Die Klassenverteilung ist **annähernd gleichverteilt**, wir haben von allen `Klassen` (etwa) gleich viele Datenpunkte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 1.2 - `sns.stripplot`, `sns.violinplot`\n",
    "\n",
    "1. Erstellen Sie einen `scatter plot` vom `Data-Set` (`df_data`) für das Feature `sepal width (cm)` und der Klasse `specie` mittels `sns.stripplot`. Der Plot heisst `stripplot` weil es die Datenpunkte vom `scatter plot` in einem Streifen (engl. `stirp`) anordnet. Würden wir das nicht machen, könnten wir die Menge der Punkte schlechter beurteilen.\n",
    "2. Wiederholen Sie Schritt 1 für alle Features. Dies kann man mittels `for` Schleife (einfacher) oder mit einem `sns.PairGrid` (schwieriger) machen. Die Spalten des DataFrames kann man mit `df_data.columns` auslesen.\n",
    "3. Interpretieren Sie den erstellten Plot.\n",
    "4. Wiederholen Sie Schritt 2 mit dem `sns.violinplot`.\n",
    "5. (Extra) Es gibt noch weitere interessante Plots wie der `sns.swarmplot` oder den `sns.boxplot`.\n",
    "6. (Extra) Was ist der Unterschied von `sns.stripplot` und `sns.violinplot`?\n",
    "\n",
    "#### Hilfreiche Links\n",
    "\n",
    "* `sns.stripplot`: https://seaborn.pydata.org/generated/seaborn.stripplot.html\n",
    "* `sns.violinplot`: https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
    "* A complete guide to plotting categorical variables: https://towardsdatascience.com/a-complete-guide-to-plotting-categorical-variables-with-seaborn-bfe54db66bec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Nur ein Feature (x und y Achse könnten nach Aufgabenstellung auch vertauscht sein)\n",
    "sns.stripplot(data=df_data, x='sepal width (cm)', y='specie', palette=colors)\n",
    "plt.show()\n",
    "\n",
    "# 2. Mit for Schleife\n",
    "for col in df_data.drop(columns='specie').columns:\n",
    "    sns.stripplot(data=df_data, x=col, y='specie', palette=colors)\n",
    "    plt.close() # Plot nicht zeigen wegen Alternative unten\n",
    "\n",
    "# 2. Alternative mit PairGrid (Plots sind in einer Zeile angeordnet)\n",
    "g = sns.PairGrid(data=df_data, x_vars=df_data.drop(columns='specie'), y_vars=['specie'])\n",
    "g.map(sns.stripplot, palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3\\.\n",
    "Die `setosa` Art hat eher kleinere Kronblätter (`petal`) und sollte einfach zu klassifizieren sein.\n",
    "Die `versicolor` und `virginica` unterscheiden und überschneiden sich ihre Verteilungen bei allen Features.\n",
    "Ein **einzelnes** Feature würde bereits die Datenpunkte ein wenig klassifizieren können (da sie sich unterscheiden), aber sicherlich nicht perfekt (da sie sich überschneiden).\n",
    "Wenn wir eine kombination von Features (z.B. alle vier Features) nehmen, kann es sein, dass `versicolor` und `virginica` in der Kombination perfekt klassifiziert werden können.\n",
    "Ob dies der Fall ist, ist hier aber nicht zu sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Mit for Schleife\n",
    "for col in df_data.drop(columns='specie').columns:\n",
    "    sns.violinplot(data=df_data, x='specie', y=col)\n",
    "    plt.close()\n",
    "\n",
    "# 4. Alternative mit PairGrid\n",
    "g = sns.PairGrid(data=df_data, x_vars=df_data.drop(columns='specie'), y_vars=['specie'])\n",
    "g.map(sns.violinplot, palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Extra: swarmplot\n",
    "g = sns.PairGrid(data=df_data, x_vars=df_data.drop(columns='specie'), y_vars=['specie'])\n",
    "g.map(sns.swarmplot, palette=colors)\n",
    "plt.show()\n",
    "\n",
    "# 5. Extra: boxplot\n",
    "g = sns.PairGrid(data=df_data, x_vars=df_data.drop(columns='specie'), y_vars=['specie'])\n",
    "g.map(sns.boxplot, palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6\\. (Extra)\n",
    "\n",
    "Der `sns.stripplot` (wie auch der `sns.swarmplot`) plotten die tatsächlichen Datenpunkte.\n",
    "Der Vorteil ist, wir sehen alle tatsächlichen Datenpunkte.\n",
    "Der Nachteil ist, bei vielen Datenpunkten kommt es zu Überschneidungen, welche schwierig zu sehen sind.\n",
    "Der `sns.violinplot` zeigt eine **Annäherung der Verteilung** des Features, viele Datenpunkte sorgen hier für kein Problem.\n",
    "Extra: Der `sns.boxplot` zeigt **Statistiken über die Verteilung**, wie den Median, Quartile und allenfalls Outliers.\n",
    "\n",
    "Im Code unten plotten wir den `sns.violinplot` (in grau) über den `sns.stripplot`, da sieht man den Zusammenhang noch einmal stark.\n",
    "Beispielsweise beim Feature `petal width (cm)` und der Spezie `setosa`, ist es anhand der Datenpunkte schwierig zu erkennen, dass es mehr Datenpunkte nahe 0 hat, als nahe 0.5, beim `sns.violinplot` ist dies aber besser zu erkennen.\n",
    "\n",
    "Achtung: Der `sns.violinplot` kann mit wenig Datenpunkten (oder falsch konfiguriert z.B. hoher `bw` Parameter) einen \"falsche\" angenäherte Verteilung erzeugen.\n",
    "Und kann zu falschen Schlussfolgerungen führen. Vorsicht ist geboten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Extra: Visualisierung von stripplot auf dem violinplot\n",
    "g = sns.PairGrid(data=df_data, x_vars=df_data.drop(columns='specie'), y_vars=['specie'])\n",
    "g.map(sns.violinplot, color=\"0.8\")\n",
    "g.map(sns.stripplot, palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlusswort Aufgabe 1\n",
    "\n",
    "Aufgabe 1 gibt Ihnen eine Idee der `Datenanalyse` für die `Classification`.\n",
    "Wir haben ein verschiedene hilfreiche Plots, wie den `countplot`, den `stripplot` und den `violinplot`  gesehen.\n",
    "Dies ist aber bei weitem nicht alles was man tun kann.\n",
    "Wir haben in Aufgabe 1.2 alle Features unabhängig von einander angeschaut, man könnte die Feature Interaktionen bereits in der Datenanalyse untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 - Logistic Regression\n",
    "\n",
    "In dieser Aufgabe erstellen wir unser erstes `Classification` Modell, ein `Logistic Regression` Modell.\n",
    "Die `LogisticRegression` ist ein `Classification`-Modell und **kein** `Regression`-Modell, trotz des irreführenden Namens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 2.1 - Plot Features - `sns.scatterplot(..., hue='col')`\n",
    "\n",
    "Wieder aus **didaktischen Gründen** (analog zu Aufgabenblatt 2) verwenden wir hier **lediglich 2 Features**, so können wir unsere Datenpunkte und trainierten Modelle 2-dimensional visualisieren.\n",
    "Die X-Achse und Y-Achse sind jeweils die Features und die Farbe der Punkte (`hue`) ist das Label der Zielvariable.\n",
    "\n",
    "Wir verwenden hier die Features `petal length (cm)` und `petal width (cm)`.\n",
    "\n",
    "1. Visualisieren Sie die beiden Features `petal length (cm)` und `petal width (cm)` vom `Data-Set` (`df_data`) mittels `sns.scatterplot` und färben sie die Punkte mit dem `hue` Parameter nach der Zielvariable `specie` ein.\n",
    "2. In Aufgabe 2.2 trainieren wir eine `LogisticRegression`, was genau wird dieses Modell schlussendlich machen? Überlegen Sie sich es am Plot aus Schritt 1.\n",
    "3. (Extra) Was ist bei den Überlegungen aus Schritt 2 der Unterschied zur Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1.\n",
    "sns.scatterplot(data=df_data, x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2\\.\n",
    "Die Logistische Regression lernt (`fit`) mehrere `Decision Boundaries`, um den Feature-Raum in `Decision Regions` zu unterteilen.\n",
    "Da die Logistic Regression linear ist, fitten wir `lineare Decision Boundaries` (`Hyper-Planes`).\n",
    "Vereinfacht gesagt trennen wir den ganzen Plot (`Feature-Raum`) mit Linien (`Hyper-Planes`) in Teilräume (`Decision Regions`).\n",
    "\n",
    "Visuell wird das dann in etwa so aussehen: Zwei Linien trennen den Plot in drei Regionen auf. Alle Punkte in einer Region werden einer Spezie z.B. `setosa` zugewiesen. (Die Linien sind hier von Auge gesetzt worden.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Handgesetzte Decision Boundaries zur Illustration')\n",
    "sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "# Diese Linien sind von Auge gesetzt, das Modell findet diese selbst.\n",
    "plt.plot([1, 3], [2.0, 0.0], c='g')\n",
    "plt.plot([4, 7], [2.5, 1.0], c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3\\. (Extra)\n",
    "In der Regression lernen (`fit`) wir eine Funktion, die einen kontinourlieren Wert zurückgibt.\n",
    "Wir haben eine zusätzliche Achse, die der Zielvariable entspricht.\n",
    "In der Classification lernen wir ein oder mehrere `Decision Boundary`, die den `Feature Raum` in `Regionen` aufteilt.\n",
    "Diese `Decision Regions` entsprechen jeweils einer möglichen `Klasse` der Zielvariable.\n",
    "Wir haben **keine extra Achse** für die Zielvariable.\n",
    "Dies ist auch der Grund wieso wir hier in der `Classification` 2 Features für die 2-dimensionale Darstellung verwenden können und in der `Regression` (Aufgabe 2 im Aufgabenblatt 2) nur 1 Feature für die 2-dimensionale Darstellung verwenden konnten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 2.2 - `sklearn.linear_model.LogisticRegression`\n",
    "\n",
    "Analog zur `LinearRegression` vom Aufgabenblatt 2, trainieren wir hier die `LogisticRegression`.\n",
    "\n",
    "1. Erstellen Sie eine `LogisticRegression` und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von `y_val_hat` unseren Vorhersagen und `y_val` den tatsächlichen `Klassen`.\n",
    "4. Visualisieren Sie die `Decision Boundary` von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?\n",
    "5. (Extra) In `sklearn.metrics` gibt es weitere Metriken, manche haben wir im Theorie Teil kennengelernt, wie den F1-Score (`f1_score`). Schauen Sie sich den `classification_report` an (Sammlung von solchen Metriken) und interpretieren Sie diese Metriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1.\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train[['petal length (cm)', 'petal width (cm)']], y_train)\n",
    "\n",
    "# 2.\n",
    "y_val_hat = clf.predict(X_val[['petal length (cm)', 'petal width (cm)']])\n",
    "\n",
    "# 3.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben eine Genauigkeit von ungefähr 96%. Also 96% der Datenpunkte wurden richtig klassifiziert und 4% wurden falsch klassifiziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "# Plotten der Decision Regions und  Decision Boundaries\n",
    "plot_decision_regions(clf, data=X_train, x='petal length (cm)', y='petal width (cm)', colors=colors)\n",
    "\n",
    "# Plotten der Datenpunkte (scatter plot)\n",
    "ax = sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "\n",
    "# Beispiel: Sage eine neue Blume mit petal length von 4cm und petal width von 0.5cm voraus\n",
    "plt.plot([4], [0.5], marker='x', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4\\.\n",
    "Im Plot sehen wir die gefundenen `Decision Boundaries` und welche Regionen zu welcher Iris-Spezie zugeordnert werden.\n",
    "Zum Beispiel, landet eine neue Blume mit `petal length (cm) = 4` und `petal width (cm) = 0.5` in der roten Region (schwarzes Kreuz **x**), also sagt das Modell `versicolor` voraus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 5. classification_report gibt viele Standard Metriken für die Classification zurück.\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\.\n",
    "Wir sehen, wie bereits schon in der Daten Analyse (Aufgabe 1.2) vermutet, dass `setosa` perfekt klassifiziert wird - `precision` und `recall` und folglich auch der `f1-score` haben den höchstwert 1.0.\n",
    "Die `precision` von 1.0 für `versicolor` bedeutet, dass wir auch alle  `versicolor` Datenpunkte im `Validation-Set` richtig klassifizieren.\n",
    "Der `recall` von 0.9 für `versicolor` bedeutet, dass wir aber auch **nicht** `versicolor` Datenpunkte im `Validation-Set` als `versicolor` klassifizieren.\n",
    "Die `precision` von 0.9 für `virginica` bedeutet, dass wir nicht alle  `virginica` Datenpunkte im `Validation-Set` richtig klassifizieren.\n",
    "Der `recall` von 1.0 für `virginica` bedeutet, dass wir, wenn wir `virginica` vorhersagen, aber richtig liegen. \n",
    "Zwischen den Blumenarten `versicolor` und `virginica` kommt es zu Fehlern, wir werden diese in der Aufgabe 2.3 mit der `Confusion Matrix` weiter \n",
    "untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.3 - Confusion Matrix\n",
    "\n",
    "In Aufgabe 2.2 haben wir ein Modell trainiert und die Genauigkeit gemessen.\n",
    "Oft ist es aber interessant, wo genau das Modell die Fehler macht.\n",
    "In der `Classification` bedeutet ein Fehler nämlich immer, dass wir einen Datenpunkt einer falschen `Klasse` zuordnen.\n",
    "Dafür haben wir im Theorie Teil die `Confusion Matrix` kennengelernt.\n",
    "\n",
    "Die Berechnung `Confusion Matrix` müssen wir nicht selbst programmieren, `sklearn` bietet bereits eine Implementierung dafür.\n",
    "\n",
    "1. Erstellen Sie mittels `sklearn.metrics.confusion_matrix` den Vorhersagen `y_val_hat` von Aufgabe 2.2 und den richtigen Labels `y_val` die `Confusion Matrix` auf dem `Validation-Set` und interpretieren Sie die `Confusion Matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 1. Apply confusion matrix (the labels parameter ensures that the order of the confusion matrix is equivalent to clf.classes_, we need it to know which row belongs to which species)\n",
    "print('Confusion Matrix from ' + ','.join(clf.classes_))\n",
    "print(confusion_matrix(y_val_hat, y_val, labels=clf.classes_))\n",
    "\n",
    "# Alternative: With sns.heatmap we can plot the confusion matrix with colors\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(y_val_hat, y_val, labels=clf.classes_),\n",
    "        columns=[clf.classes_], # Name columns\n",
    "        index=[clf.classes_] # Name rows\n",
    "    ),\n",
    "    annot=True # Show numbers in heatmap (not just colors)\n",
    ")\n",
    "plt.ylabel('True label') # Name y-axis\n",
    "plt.xlabel('Predicted label') # Name x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1\\.\n",
    "Die `Confusion Matrix` gibt an, wo wir richtig Vorhersagen (Diagonale) und wo wir falsch Vorhersagen (Rest) machen.\n",
    "Hier betrachten wir die `Confusion Matrix` vom `Validation-Set`.\n",
    "Unser Modell ist berets sehr gut (es ist ein einfacher Datensatz):\n",
    "* Wir sagen 9 Mal `setosa` korrekt voraus.\n",
    "* Wir sagen 9 Mal `versicolor` korrekt voraus.\n",
    "* Wir sagen 9 Mal `virginica` korrekt voraus.\n",
    "* Wir machen nur 1 falsche Vorhersage: Für 1 Datenpunkt sagen wir `virginica` anstatt `versicolor` voraus.\n",
    "\n",
    "Die Nullen sind folgendermassen zu interpretieren, beispielsweise für die 0 in der ersten Zeile und der zweiten Spalte: Wir sagen 0 Mal `versicolor` anstatt `setosa` voraus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Aufgabe 2.4 - Was wurde gelernt?\n",
    "\n",
    "Die `Logistische Regression` hat folgende Form:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "    \\phi(x^{(i)}\\beta) = \\phi(\\beta_0 + x_1 \\beta_1 + \\cdots + x_p \\beta_p)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Dies ist einfach die `Lineare Regression` mit der Sigma-Funktion $\\phi$ als `Linker-Funktion`.\n",
    "\n",
    "Da wir nur zur Zeit nur zwei Features haben, vereinfacht sich das zu:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "    \\phi(x^{(i)}\\beta) = \\phi(\\beta_0 + x_1 \\beta_1 + x_2 \\beta_2)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Analog zum Aufgabenblatt 2 können wir auch nachschauen, welche $\\beta$s (Gewichte) gelernt wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"beta0\", clf.intercept_)\n",
    "print(\"beta1\", clf.coef_[:, 0])\n",
    "print(\"beta2\", clf.coef_[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Es wurden drei verschiedene $\\beta_0$, $\\beta_1$ und $\\beta_2$ gelernt!\n",
    "\n",
    "1\\.\n",
    "Warum ist das? Was bedeutet das?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1\\.\n",
    "Die klassische Logistische Regression $\\phi(x^{(i)}\\beta) = \\phi(\\beta_0 + x_1 \\beta_1 + \\cdots + x_p \\beta_p)$ ist ein `Binärer-Classifier`, sie kann nur 2 `Klassen` unterscheiden.\n",
    "`sklearn` bietet mit dem `multi_class` Parameter verschiedene Strategien an, um mit mehreren `Klassen` umzugehen.\n",
    "* `auto`: Entscheidet automatisch (default `multinomial`)\n",
    "* `ovr`: One-Vs-Rest (Theoretischer Teil)\n",
    "* `multinomial`: (Extra) https://en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
    "\n",
    "Wir haben in unserem Datensatz 3 `Klassen` (`setosa`, `versicolor`, `virginica`) und haben der `LogisticRegression` von sklearn auch alle 3 Klassen übergeben. Ohne den `multi_class` Parameter macht die `LogisticRegression` von `sklearn` einfach eine `multinomial` Behandlung.\n",
    "\n",
    "Bei `ovr` werden explizit 3 Modelle trainiert\n",
    "* Ein `seposa`-vs-Rest Modell\n",
    "* Ein `versicolor`-vs-Rest Modell\n",
    "* Ein `virginica`-vs-Rest Modell\n",
    "\n",
    "Bei `multinomial` trainiere wir 3 Modell-Teile zusammen (mit einer gemeinsamen Cost-Function (z.B. Softmax)):\n",
    "* Ein `seposa` Teil\n",
    "* Ein `versicolor` Teil\n",
    "* Ein `virginica` Teil\n",
    "\n",
    "Darum haben wir also 3 Parameter-Paare (3 Intercepts ($\\beta_0$) und 3 mal 2 Koeffizienten ($\\beta_1$, $\\beta_2$)).\n",
    "\n",
    "Wir haben `One-vs-Rest` im theoretischen Teil genauer angeschaut.\n",
    "Hier plotten wir die 3 **zugrundeliegenden Modelle** in der jeweiligen Farbe.\n",
    "(Dies war nicht Teil der Aufgabe.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Logistic Regression mit One-Vs-Rest\n",
    "clf = LogisticRegression(multi_class='ovr')\n",
    "clf.fit(X_train[['petal length (cm)', 'petal width (cm)']], y_train)\n",
    "\n",
    "# Plotten der Decision Regions und  Decision Boundaries\n",
    "plot_decision_regions(clf, data=X_train, x='petal length (cm)', y='petal width (cm)', colors=colors)\n",
    "ax = sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "\n",
    "[x_min, x_max] = ax.get_xlim()\n",
    "[y_min, y_max] = ax.get_ylim()\n",
    "\n",
    "def plot_hyperplane(c: int, color: str):\n",
    "    def line(x0):\n",
    "        return (-(x0 * clf.coef_[c, 0]) - clf.intercept_[c]) / clf.coef_[c, 1]\n",
    "    plt.plot([x_min, x_max], [line(x_min), line(x_max)], color=color)\n",
    "\n",
    "for specie, color in colors.items():\n",
    "    class_index = list(clf.classes_).index(specie)\n",
    "    plot_hyperplane(class_index, color)\n",
    "\n",
    "plt.legend([specie + '-vs-Rest' for specie in colors.keys()])\n",
    "\n",
    "ax.set_xlim([x_min, x_max])\n",
    "ax.set_ylim([y_min, y_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Die <span style=\"color:green\">grüne Linie</span> ist das `seposa-vs-Rest` Modell. Es kann die `seposa` Datenpunkte (grün) gut vom Rest (rot und blau) trennen.\n",
    "\n",
    "Die <span style=\"color:blue\">blau Linie</span> ist das `seposa-vs-Rest` Modell. Es kann die `virginica` Datenpunkte (blau) gut vom Rest (rot und grün) trennen.\n",
    "\n",
    "Die <span style=\"color:red\">rote Linie</span> ist das `versicolor-vs-Rest` Modell, es macht einen Kompromiss, da die Datenpunkte (rot) nicht gut **linear** vom Rest (blau und grün) trennen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlusswort Aufgabe 2\n",
    "\n",
    "In Aufgabe 2 haben wir unser erstes `Classification` Modell, eine `Logistische Regression`, angewandt.\n",
    "In den nächsten Aufgaben werden wir weitere `Classification` Modelle anwenden.\n",
    "\n",
    "Für die Auswertung unseres Modelles nutzten wir `Classification` Metriken, wie die `Genauigkeit` (Accuracy) \n",
    "und die `Confusion Matrix`.\n",
    "Diese Auswertungen können wir genau gleich wieder bei anderen Modellen anwenden.\n",
    "\n",
    "Der Iris-Datensatz ist bereits **linear gut separierbar**, mit nur 2 Featuren und ohne `Feature Engineering` und ohne nicht-lineare Modelle.\n",
    "Obwohl der Datensatz (zu) einfach ist bleiben wir auch für die weiteren Aufgaben auf dem Iris-Datensatz.\n",
    "Es geht in den weiteren Aufgaben mehr um die Visualisierungen der Modelle, sowie dem Verständnis wie sie sich unterscheiden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3 - Support Vector Machine\n",
    "\n",
    "In Aufgabe 3 wenden wir `Support Vector Machinen` für die `Classification` an (`sklearn.svm.SVC`).\n",
    "`Support Vector Machinen` könnten auch für die Regression benutzt werden (`sklearn.svm.SVR`), daher der Name `SVC` im `sklearn`.\n",
    "\n",
    "### Aufgabe 3.1 - Linear Support Vector Machine\n",
    "\n",
    "Analog zur Aufgabe 2.2 trainieren wir unser Modell vorerst auf 2 Featuren: `petal length (cm)` und `petal width (cm)`.\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.svm.SVC` mit dem linearen Kernel (`kernel='linear'`) und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die decision boundary von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 1.\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train[['petal length (cm)', 'petal width (cm)']], y_train)\n",
    "\n",
    "# 2.\n",
    "y_val_hat = clf.predict(X_val[['petal length (cm)', 'petal width (cm)']])\n",
    "\n",
    "# 3.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "# Plotten der Decision Regions und  Decision Boundaries\n",
    "\n",
    "plot_decision_regions(clf, data=X_train, x='petal length (cm)', y='petal width (cm)', colors=colors)\n",
    "\n",
    "# Plotten der Datenpunkte (scatter plot)\n",
    "sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\.\n",
    "Der Plot ist analog zur Aufgabe 2.2 die `Decision Region` unseres `Classification` Modelles. Anders als in Aufgabe 2.2 handelt es sich hier um eine `Support Vector Machine`, daher unterscheiden sich die `Decision Regions`. Der Unterschied wird in Aufgabe 3.2 erklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 3.2 - Logistische Regression oder Support Vector Machine\n",
    "\n",
    "Im theoretischen Teil haben wir gesehen, wie die `Logistische Regression` und wie die `Support Vector Machine` hinter den Kullisen (innerhalb der `fit` Methode) trainiert werden.\n",
    "Nutzen Sie Ihr Wissen in folgender Aufgabe.\n",
    "\n",
    "Unten sehen Sie zwei Visualisierungen der `Decision Regions` von Aufgabe 2.2 und Aufgabe 3.1 \n",
    "\n",
    "Logistische Regression     |  Support Vector Machine\n",
    ":-------------------------:|:-------------------------:\n",
    "![Logistische Regression oder SVM?](./img/lr_or_svm_2.png) |  ![Logistische Regression oder SVM?](./img/lr_or_svm_1.png)  \n",
    "\n",
    "1. Woran man das das zugrundeliegende Modell anhand der `Decision Regions` erkennen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1\\.\n",
    "\n",
    "Die `Logistische Regression` setzt die `Decision Boundary` zwischen `setosa` und `versicolor` betreffend allen Datenpunkten - die `Decision Boundary` geht hier nahe an einem `versicolor` Datenpunkt vorbei.\n",
    "\n",
    "Die `Support Vector Machine` setzt die `Decision Boundary` betreffend den `Support Vectoren` (nahsten Datenpunkten), sie versucht den Abstand (`Margin`) zu nahsten Datenpunkt zu maximieren - die `Decision Boundary` ist folglich weit entfernt von allen `setosa` und `versicolor` Datenpunkten. \n",
    "\n",
    "Nehmen wir an neue Datenpunkte sind nahe den Datenpunkten im `Train-Set`, ist die Trennung der `Support Vector Machine` in diese Fall wahrscheinlich besser im Verallgemeinern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3.3 - Support Vector Machine - Kernel Trick - Nicht Lineares Modell\n",
    "\n",
    "Im theoretischen Teil haben wir den `Kernel Trick` erwähnt, den wir in der `Support Vector Machine` anwenden können, um ein nicht lineares Modell zu erhalten. Generell kann man diesen Trick in verschiedenen Modellen einsetzen (Lineare Regression, Logistische Regression, KNN, ...), er ist jedoch nur bei `Support Vector Machine` out-of-the-box in sklearn verfügbar.\n",
    "\n",
    "Welcher Kernel wir verwenden und welche Parameter wir für einen spezifischen Kernel verwenden sind `Hyper-Parameter` vom `Support Vector Machine` Modell.\n",
    "\n",
    "Wir können diese `Hyper-Parameter` manuell (mit Expertenwissen) setzen oder systematisch (mit `Hyper-Parameter Optimization` (Aufgabe 3.4)) finden.\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.svm.SVC` mit dem `rbf` Kernel (`kernel='rbf'`) und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die `Accuracy` mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die `Decision Boundary` von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was ist grundlegend anders als in der Visualisierung der `linearen Support Vector Machine` von Aufgabe 3.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 1.\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(X_train[['petal length (cm)', 'petal width (cm)']], y_train)\n",
    "\n",
    "# 2.\n",
    "y_val_hat = clf.predict(X_val[['petal length (cm)', 'petal width (cm)']])\n",
    "\n",
    "# 3.\n",
    "print(accuracy_score(y_val_hat, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "# Plotten der Decision Regions und  Decision Boundaries\n",
    "\n",
    "plot_decision_regions(clf, data=X_train, x='petal length (cm)', y='petal width (cm)', colors=colors)\n",
    "\n",
    "# Plotten der Datenpunkte (scatter plot)\n",
    "sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\.\n",
    "\n",
    "Die `Decision Boundaries` sind dank dem `Kernel Trick` **nicht mehr linear**. Vorallem bei der `Decision Boundary` von `versicolor` und `virginica` sehen wir eine leichte Krümmung. \n",
    "\n",
    "Der `gamma` Parameter vom `rbf` Kernel gibt vereinfacht gesagt an \"wie nicht linear\" wir sind, wir können z.B. `gamma` auf 100 setzen, was das Modell flexibler macht und leider zu einem `Overfitting` führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='rbf', gamma=100)\n",
    "clf.fit(X_train[['petal length (cm)', 'petal width (cm)']], y_train)\n",
    "y_val_hat = clf.predict(X_val[['petal length (cm)', 'petal width (cm)']])\n",
    "plot_decision_regions(clf, data=X_train, x='petal length (cm)', y='petal width (cm)', colors=colors)\n",
    "sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "print(\"Wir overfitting: \", accuracy_score(y_val_hat, y_val))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3.4 - Support Vector Machine - Alle Features und Hyper-Parameter Optimization\n",
    "\n",
    "Nun möchten wir alle Features verwenden und den Hyper-Parameter `gamma` vom `rbf` Kernel mittels `Hyper-Parameter Optimierung` finden.\n",
    "\n",
    "Dazu können wir unterschiedliche Such-Strategien verwenden:\n",
    "* `Grid Search`: Systematisch ein Grid absuchen\n",
    "* `Randomized Search`: `n` Mal zufälle Parameter in einem fixen Bereich ausprobieren\n",
    "\n",
    "Wir verwenden hier `Randomized Search`, da es in der Praxis erstaunlich oft gut funktioniert.\n",
    "`sklearn` bietet bereits eine Implementierung mit `Cross Validation` für stabilere Aussagen für die einzelnen Parameter bereit namens `RandomizedSearchCV`.\n",
    "\n",
    "1. Erstellen Sie einen fixen Bereich für den `gamma` Parameter, analog zu diesem Beispiel.\n",
    "2. Erstellen Sie eine `RandomizedSearchCV` mit einer `SVC(kernel='rbf')`, dem in Schritt 1 erstellten fixen Bereich. Wählen Sie eine sinnvolle Anzahl an Iterationen (z.B. `n_iter=25`).\n",
    "3. Trainieren `fit` Sie die `RandomizedSearchCV` auf dem `Data-Set` (`X_data` `y_data`).\n",
    "4. Warum nehmen wir in Schritt 3 das `Data-Set` und nicht das `Train-Set`?\n",
    "5. Welches Hyper-Parameter wurden gefunden (hier `gamma`)? Verwenden Sie dazu `rscv.best_params_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "# 1.\n",
    "param_dist = dict(\n",
    "    gamma=loguniform(1e-5, 10),\n",
    ")\n",
    "\n",
    "# 2.\n",
    "rs_cv = RandomizedSearchCV(SVC(kernel='rbf'), param_dist, n_iter=25)\n",
    "\n",
    "# 3. \n",
    "# Dieser fit call trainiert für 25 zufällige gamma Werte jeweils 3 SVC Modelle und evaluiert, welches zufällige gamma im Durchschnitt am beste war.  \n",
    "rs_cv.fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\.\n",
    "`RandomizedSearchCV` führt intern `Cross Validations` durch, also macht es den `Train-Set`, `Validation-Set` Split bereits intern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n",
    "print(rs_cv.best_params_)\n",
    "print(rs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Extra) Aufgabe 4 - Weitere Modelle\n",
    "\n",
    "Analog zur Aufgabe 2.2 (Logistische Regression) und Aufgabe 3.1 (Support Vector Machine), trainieren wir hier andere Classification-Modelle, um ein Gefühl für den Unterschied zu bekommen.\n",
    "\n",
    "### (Extra) Aufgabe 4.1 - RandomForestClassifier\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.ensemble.RandomForestClassifier` und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die decision boundary von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1.\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train[['petal length (cm)', 'petal width (cm)']], y_train)\n",
    "\n",
    "# 2.\n",
    "y_val_hat = clf.predict(X_val[['petal length (cm)', 'petal width (cm)']])\n",
    "\n",
    "# 3.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_val_hat, y_val))\n",
    "\n",
    "# 4.\n",
    "plot_decision_regions(clf, data=X_train, x='petal length (cm)', y='petal width (cm)', colors=colors)\n",
    "sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4\\.\n",
    "Binäre-Entscheidungsbäume teilen pro Knoten eine Feature-Achse in zwei Regionen auf.\n",
    "`RandomForest` ist eine Ansammlung (`Ensemble`) von solchen Binären-Entscheidungsbäumen.\n",
    "Wir sehen die harten Entscheidungen von den zugrundeliegenden Entscheidungsbäumen, mit klaren vertikalen und horizontalen Teilen noch in der finalen `Decision Boundary`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (Extra) Aufgabe 4.2 - KNN (K-Nearest-Neighbors)\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.neighbors.KNeighborsClassifier` und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die decision boundary von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 1.\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train[['petal length (cm)', 'petal width (cm)']], y_train)\n",
    "\n",
    "# 2.\n",
    "y_val_hat = clf.predict(X_val[['petal length (cm)', 'petal width (cm)']])\n",
    "\n",
    "# 3.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_val_hat, y_val))\n",
    "\n",
    "# 4.\n",
    "plot_decision_regions(clf, data=X_train, x='petal length (cm)', y='petal width (cm)', colors=colors)\n",
    "sns.scatterplot(data=pd.concat([X_data, y_data], axis=1), x='petal length (cm)', y='petal width (cm)', hue='specie', palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4\\.\n",
    "`KNN` trainiert gar keine Parameter. Im `fit` Schritt speichert es das `Train-Set` für effizientes Abfragen in einer Baum-Datenstruktur ab.\n",
    "Bei der Vorhersage (`predict`) sucht es einfach die `k` nahsten Nachbarn und sagt deren häufigste Klasse voraus.\n",
    "Durch diesen Prozess erhalten wir ein nicht lineares Modell.\n",
    "Mit vielen Features leidet `KNN` aber stark unter dem `Curse of Dimensionality` und kann oft schlecht verallgemeinern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 5 - Test Set\n",
    "\n",
    "Im Setup haben wir das `Test-Set` (`X_test`, `y_test`) erstellt und beiseite gelegt. Nun wollen wir das gefundene Modell auf diesem `Test-Set` evaluieren.\n",
    "\n",
    "1. Nehmen Sie ein Modell aus einer vorherigen Aufgabe als finales Modell.\n",
    "2. Wenden Sie dieses Modell auf dem `Test-Set` (`X_test`, `y_test`) an. `X_test` muss allenfalls noch entsprechend verarbeitet werden.\n",
    "3. Bestimmen Sie die Genauigkeit auf den Vorhersagen von Schritt 2 und `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Wir nehmen hier das Modell vom Aufgabe 3.4\n",
    "final_model = rs_cv\n",
    "\n",
    "# 2.\n",
    "y_test_hat = final_model.predict(X_test)\n",
    "\n",
    "# 3.\n",
    "print(accuracy_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Eine Genauigkeit von 1.0 bedeutet wir haben alle Datenpunkte (100%) im `Test-Set` richtig kategorisiert - besser geht es nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Schlusswort Aufgabenblatt 3\n",
    "\n",
    "Im Aufgabenblatt 3 haben wir die `Classification` genauer angeschaut.\n",
    "Wir haben verschiedene Plots für die `Datenanalyse`, verschiedene `Metriken` und verschiedene Modelle angeschaut.\n",
    "Es ist vorallem wichtig, dass Sie ein erstes Gefühl für die `Classification` bekommen haben und die Unterschiede zur Regression (Aufgabenblatt 2) klar verstehen.\n",
    "\n",
    "Der verwendete Iris-Datensatz ist ein oft verwendeter erster Datensatz. In der Praxis hat man oft schwierigere Datensätze, dass man eine Genauigkeit von 100% erreicht ist eher unüblich und je nach Problem unmöglich."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}